{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "# set some globals\n",
    "DATA_DIR = Path(\"/home/rhamilton/code/cnwi/test_data/aoi_NS/data\")\n",
    "\n",
    "# set the Data for the images\n",
    "@dataclass\n",
    "class Payload:\n",
    "    \"\"\"Payload for the datacube\n",
    "    s1: Sentinel-1\n",
    "    dc: Data cube Composites\n",
    "    al: ALOS\n",
    "    ft: Fourier Transform\n",
    "    ta: Terrain Analysis\n",
    "    \"\"\"\n",
    "    s1 = [\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190601T220203_20190601T220228_027492_031A28_EB74\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190601T220228_20190601T220253_027492_031A28_1D62\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190601T220253_20190601T220318_027492_031A28_B0EC\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190601T220318_20190601T220343_027492_031A28_3A0C\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190731T220207_20190731T220232_028367_0334A1_0ECA\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190731T220232_20190731T220257_028367_0334A1_32FF\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190731T220257_20190731T220322_028367_0334A1_4F99\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190731T220322_20190731T220347_028367_0334A1_7758\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190606T221027_20190606T221052_027565_031C53_C63B\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190606T221052_20190606T221117_027565_031C53_1088\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190606T221117_20190606T221142_027565_031C53_7FD5\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190606T221142_20190606T221207_027565_031C53_E704\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190805T221031_20190805T221056_028440_0336C5_08AD\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190805T221056_20190805T221121_028440_0336C5_8F55\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190805T221121_20190805T221146_028440_0336C5_322A\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190805T221146_20190805T221211_028440_0336C5_0053\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190527T215404_20190527T215429_027419_0317D3_FF9D\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190527T215429_20190527T215454_027419_0317D3_48EB\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190527T215454_20190527T215519_027419_0317D3_1009\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190527T215519_20190527T215544_027419_0317D3_501E\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190726T215408_20190726T215433_028294_033253_09E6\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190726T215433_20190726T215458_028294_033253_92C9\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190726T215458_20190726T215523_028294_033253_8B6E\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190726T215523_20190726T215548_028294_033253_7A46\",\n",
    "        \"COPERNICUS/S1_GRD/S1B_IW_GRDH_1SDV_20190605T221811_20190605T221836_016567_01F302_2750\",\n",
    "        \"COPERNICUS/S1_GRD/S1B_IW_GRDH_1SDV_20190605T221836_20190605T221901_016567_01F302_AFE0\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190729T221905_20190729T221934_028338_0333B8_EEBB\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190729T221934_20190729T221959_028338_0333B8_1E30\",\n",
    "        \"COPERNICUS/S1_GRD/S1A_IW_GRDH_1SDV_20190729T221959_20190729T222024_028338_0333B8_BD9F\",\n",
    "    ]\n",
    "    dc = \"projects/fpca-336015/assets/cnwi-datasets/aoi_novascotia/datacube\"\n",
    "    ft = \"projects/fpca-336015/assets/NovaScotia/fourier_transform\"\n",
    "    ta = \"projects/fpca-336015/assets/NovaScotia/terrain_analysis\"\n",
    "    al = \"JAXA/ALOS/PALSAR/YEARLY/SAR_EPOCH\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client side processing must be done first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class_name                    geometry  type region_id  value\n",
      "0      Swamp  POINT (-62.48171 45.03326)   2.0       127      1\n",
      "1      Swamp  POINT (-62.54104 45.01055)   2.0       127      1\n",
      "2      Swamp  POINT (-63.36111 44.89482)   2.0       127      1\n",
      "3      Swamp  POINT (-63.51164 44.85164)   2.0       127      1\n",
      "4      Swamp  POINT (-63.65494 44.84893)   2.0       127      1\n"
     ]
    }
   ],
   "source": [
    "# TODO should write out reference data to a csv file i.e code books for types etc\n",
    "# This is the inital processing step for the features chain\n",
    "# need to get the training data from the client\n",
    "from cnwi.cnwilib.data import *\n",
    "from cnwi.cnwilib.features import insert_values_into_features\n",
    "\n",
    "shp_files = get_shapefile_paths(DATA_DIR)\n",
    "manifest = create_raw_data_manifest(shp_files)\n",
    "\n",
    "# save the manifest\n",
    "if not Path(DATA_DIR.parent / 'manifest').exists():\n",
    "    Path(DATA_DIR.parent / 'manifest').mkdir()\n",
    "\n",
    "manifest.to_csv(DATA_DIR.parent / 'manifest' / 'manifest.csv')\n",
    "\n",
    "# process the manifest\n",
    "gdf_all = process_data_manifest(manifest) # creates a geodataframe for all regions\n",
    "if gdf_all.crs != 'EPSG:4326':\n",
    "    gdf_all.to_crs('EPSG:4326', inplace=True)\n",
    "# TODO need to insert a value column for the training data\n",
    "# create the lookup for all regions\n",
    "lookup = create_lookup_table(gdf_all)\n",
    "gdf_all = insert_values_into_features(gdf_all, lookup)\n",
    "print(gdf_all.head())\n",
    "# save the lookup\n",
    "if not Path(DATA_DIR.parent / 'reference').exists():\n",
    "    Path(DATA_DIR.parent / 'reference').mkdir()\n",
    "\n",
    "lookup.to_csv(DATA_DIR.parent / 'reference' / 'lookup.csv')\n",
    "\n",
    "# save all the data\n",
    "if not Path(DATA_DIR / 'processed').exists():\n",
    "    Path(DATA_DIR / 'processed').mkdir()\n",
    "gdf_all.to_file(DATA_DIR / 'processed' / 'all_regions.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to process the files further\n",
    "from typing import Generator, Tuple\n",
    "def compute_by_region(manifest, gdf_all) -> Generator[Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame], None, None]:\n",
    "    \"\"\"Compute by region\n",
    "    \"\"\"\n",
    "    for _, group in manifest.groupby('region_id'):\n",
    "        # get the region id\n",
    "        region = gpd.read_file(group[group[\"type\"] == 3][\"file_path\"].values[0])\n",
    "        if region.crs != \"EPSG:4326\":\n",
    "            region.to_crs(epsg=4326, inplace=True)\n",
    "        # now get the training data\n",
    "        training = gdf_all[gdf_all[\"region_id\"] == _ ]\n",
    "        yield (region, training[['value', 'type', 'geometry']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "region, gdf = next(compute_by_region(manifest, gdf_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POINT (-64.20332 45.87357)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POINT (-63.00702 45.70105)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POINT (-64.13768 46.10270)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POINT (-63.64956 45.80755)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POINT (-63.03562 45.70061)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value  type                    geometry\n",
       "1348      1   2.0  POINT (-64.20332 45.87357)\n",
       "1349      1   2.0  POINT (-63.00702 45.70105)\n",
       "1350      1   2.0  POINT (-64.13768 46.10270)\n",
       "1351      1   2.0  POINT (-63.64956 45.80755)\n",
       "1352      1   2.0  POINT (-63.03562 45.70061)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to maintain the index \n",
    "# the probelm im going to run into is putting the subseted data back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the data for the images\n",
    "class ImageCollectionProc:\n",
    "    def __init__(self, arg: str | list[str])  -> None:\n",
    "        self.collection = arg\n",
    "    \n",
    "    @property\n",
    "    def collection(self) -> ee.ImageCollection:\n",
    "        return self._collection\n",
    "    \n",
    "    @collection.setter\n",
    "    def collection(self, arg: str | list[str]) -> None:\n",
    "        self._collection = ee.ImageCollection(arg)\n",
    "    \n",
    "    def run(self, aoi, start: str = None, end: str = None) -> ee.ImageCollection:\n",
    "        \"\"\"Run the image collection processing\n",
    "        Args:\n",
    "            aoi: Area of Interest\n",
    "            start: start date\n",
    "            end: end date\n",
    "        Returns:\n",
    "            ee.ImageCollection\n",
    "        \"\"\"\n",
    "        if start and end:\n",
    "            return self.collection.filterBounds(aoi).filterDate(start, end)\n",
    "        else:\n",
    "            return self.collection.filterBounds(aoi)\n",
    "\n",
    "\n",
    "# the data sets will be in a Image Collection for intial processing\n",
    "\n",
    "# in SciOps the Images are more or less hand selected therefore minimal processing is needed at\n",
    "# the image collection stage, left in an image collection b/c it is easier to isolate the images\n",
    "# needed by region\n",
    "\n",
    "# this is the inital processing step for the images in the chain\n",
    "pyld = Payload()\n",
    "dcc = ImageCollectionProc(pyld.dc).run()\n",
    "s1c = ImageCollectionProc(pyld.s1).run()\n",
    "alc = ImageCollectionProc(pyld.al).run()\n",
    "tac = ImageCollectionProc(pyld.ta).run()\n",
    "ftc = ImageCollectionProc(pyld.ft).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will need to build the data sets for the models, this is were a majoirty of the processing\n",
    "# happens\n",
    "\n",
    "from cnwi.cnwilib.image import ImageBuilder, ImageDirector, ImageStack\n",
    "\n",
    "# create a stack object\n",
    "stack = ImageStack() # storage Container for the images\n",
    "\n",
    "# optical inputs\n",
    "dc_bldt = ImageBuilder(dcc)\n",
    "dc_dir = ImageDirector(dc_bldt).build_data_cube()\n",
    "stack.add(dc_dir.builder.image)\n",
    "\n",
    "# SAR inputs\n",
    "s1_bldt = ImageBuilder(s1c)\n",
    "s1_dir = ImageDirector(s1_bldt).build_sentinel_1()\n",
    "stack.add(s1_dir.builder.image)\n",
    "\n",
    "# AL inputs\n",
    "al_bldt = ImageBuilder(alc)\n",
    "al_dir = ImageDirector(al_bldt).build_alos()\n",
    "stack.add(al_dir.builder.image)\n",
    "\n",
    "# TA inputs\n",
    "stack.add(tac.mosaic()) # processing done externally\n",
    "\n",
    "# FT inputs\n",
    "stack.add(ftc.mosaic()) # processing done externally\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
